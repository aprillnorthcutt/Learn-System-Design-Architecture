# ğŸ“ System Design Estimation Sheet (Back-of-the-Envelope)

> Use this README to estimate **latency, throughput, concurrency, threads, and bandwidth** â€” quickly, clearly, and ADHD-friendly.

---

## ğŸ§­ Core Mental Models

- **Littleâ€™s Law:**  
  `Concurrency â‰ˆ Throughput Ã— Latency`  
  (aka **R = Î» Ã— W** â†’ Requests in flight = RPS Ã— avg response time)

- **Critical Path Latency:**  
  `Sequential total = A + B + C` ; `Parallel total = max(A,B,C)`

- **SLA Budgeting:**  
  Give each stage a time budget that sums to your SLA (e.g., 300â€¯ms).

---

## â±ï¸ Latency Budget Template

Break your API into **Ping â†’ Think â†’ Fetch â†’ Return** and fill in blanks:

| Stage | Budget (ms) | Notes |
|---|---:|---|
| Client â†” Server (network) | ____ | DNS/TCP/TLS + RTT |
| App logic (CPU) | ____ | Auth, routing, business logic |
| Cache / DB | ____ | Cache hit (1â€“5â€¯ms), DB (20â€“100+â€¯ms) |
| Downstream calls | ____ | Count hops; parallelize where possible |
| Serialization + return | ____ | JSON/proto, compression |
| **Total** | **â‰¤â€¯SLA** | Aim â‰¤â€¯300â€¯ms for interactive |

> ğŸ’¡ **Tip:** Keep a 10â€“20% buffer for jitter.

---

## ğŸš¦ Throughput, Latency, Concurrency

- **Given RPS and latency, estimate concurrency:**
  ```
  Concurrency (in-flight) = RPS Ã— Latency_seconds
  ```
  Example: 600â€¯RPS Ã— 0.25â€¯sâ€¯=â€¯**150 concurrent requests**

- **Given max concurrency and latency, max safe RPS:**
  ```
  RPS_max â‰ˆ Concurrency / Latency_seconds
  ```
  Example: 200â€¯threads /â€¯0.25â€¯sâ€¯â‰ˆâ€¯**800â€¯RPS**

- **Translate concurrency to threads (blocking model):**
  ```
  Threads_needed â‰ˆ Concurrency Ã— overhead_factor
  ```
  Example: 150â€¯concurrent â†’â€¯~180â€¯threads (1.2Ã—â€¯overhead)

> âš ï¸ **Watch out:** If I/Oâ€‘bound, prefer async/reactive instead of many threads.

---

## ğŸ”¢ QPS â†” CPU / Instance Count

Assume each instance can handle **Xâ€¯RPS** at target P95 (e.g.,â€¯120â€¯RPSâ€¯@â€¯â‰¤â€¯250â€¯ms):
```
Instances_needed â‰ˆ ceil( Total_RPS / RPS_per_instance Ã— headroom )
```
Example: 2400â€¯RPSâ€¯Ã·â€¯120â€¯Ã—â€¯1.25â€¯=â€¯**25â€¯instances**

> ğŸ’¡ **Headroom:**â€¯1.2â€“1.5 for traffic bursts.

---

## ğŸŒ Bandwidth & Payload Math

- **Request transfer time:**
  ```
  Transfer_ms â‰ˆ (Payload_bytes Ã— 8 / Link_bps) Ã— 1000
  ```
  Example:â€¯512â€¯KBâ€¯@â€¯100â€¯Mbpsâ€¯â†’â€¯**42â€¯ms**â€¯(minimum)

- **Link saturation:**
  ```
  Bandwidth_bps â‰ˆ RPS Ã— Payload_bytes Ã— 8
  ```
  Example:â€¯1000â€¯RPS Ã—â€¯50â€¯KBâ€¯Ã—â€¯8â€¯=â€¯**400â€¯Mbps**

> ğŸ’¡ Keep NIC <â€¯70%â€¯utilization; compress >â€¯10â€¯KB payloads.

---

## ğŸ§° DB Capacity & Tail Latency

- **Max QPS:**
  ```
  QPS_max â‰ˆ Concurrency_limit / Latency_seconds
  ```
  Example:â€¯50â€¯activeâ€¯/â€¯0.02â€¯=â€¯2500â€¯QPS

> âš ï¸ **Rule of thumb:**â€¯P99â€¯â‰ˆâ€¯3â€“5Ã—â€¯avg latency.

---

## ğŸ“¦ Cache ROI (Miss Rate)

- **Backend load:**
  ```
  Backend_QPS = Incoming_QPS Ã— Miss_rate
  ```
- **Average latency:**
  ```
  L_avg = Hit_rate Ã— L_hit + Miss_rate Ã— L_miss
  ```
  Example:â€¯90%â€¯hitsâ€¯(3â€¯ms)â€¯+â€¯10%â€¯missesâ€¯(80â€¯ms)â€¯=â€¯**10.7â€¯ms**â€¯average.

---

## ğŸ” Retries & Timeouts

- **Retry multiplier:**â€¯`Effective_RPS = Incoming_RPS Ã— (1 + retry_rate)`  
  â†’â€¯2%â€¯retriesâ€¯=â€¯1.02Ã—â€¯load

- **Timeout budgeting:**â€¯Split SLA among attemptsâ€¯(e.g.,â€¯300â€¯msâ€¯â†’â€¯140â€¯msâ€¯Ã—â€¯2â€¯+â€¯buffer)

> ğŸ”„ Add **jitter** to backoff to avoid thundering herd.

---

## ğŸªµ Fan-Out / Fan-In Latency

| Pattern | Formula | Example |
|---|---|---|
| Sequential | Î£â€¯steps | 80â€¯+â€¯120â€¯+â€¯60â€¯=â€¯260â€¯ms |
| Parallel | max(steps)â€¯+â€¯overhead | max(80,â€¯120,â€¯60)â€¯+â€¯5â€¯=â€¯**125â€¯ms** |

---

## ğŸ§ª Worked Example (E2E)

**Targetâ€¯SLA:**â€¯300â€¯ms  
**Flow:**â€¯RESTâ€¯APIâ€¯â†’â€¯Cacheâ€¯â†’â€¯DBâ€¯+â€¯Externalâ€¯APIâ€¯(Parallel)

| Step | Timeâ€¯(ms) |
|---|---:|
| Networkâ€¯RTT |â€¯120 |
| Appâ€¯Logic |â€¯30 |
| Cacheâ€¯/â€¯DB |â€¯60 |
| Externalâ€¯APIâ€¯(Parallel) |â€¯80 |
| Serializationâ€¯+â€¯Return |â€¯30 |
| **Total** |â€¯â‰ˆâ€¯**260â€¯ms** âœ… |

**Throughput:**â€¯1800â€¯RPSâ€¯Ã—â€¯0.26â€¯sâ€¯=â€¯468â€¯inâ€‘flightâ€¯â†’â€¯~6â€¯instancesâ€¯(120â€¯perâ€¯instanceâ€¯Ã—â€¯1.5â€¯headroom)

---

## ğŸ§© Quick Reference Tables

### Latency Magnitudes
| Operation | Ballpark |
|---|---|
| Inâ€‘memory (Redis/Memcached) | 1â€“5â€¯ms |
| Localâ€¯DBâ€¯(SSD) | 5â€“20â€¯ms |
| Remoteâ€¯DBâ€¯/â€¯Crossâ€‘AZ | 50â€“100+â€¯ms |
| Microservice hop | 10â€“50â€¯ms |
| Userâ†”DC RTT | 100â€“150â€¯ms |
| Feels â€œinstantâ€ | <â€¯300â€¯msâ€¯E2E |

### Kafkaâ€‘Style Producer/Consumer Tuning
| Goal | Knob | Direction |
|---|---|---|
| Lowerâ€¯p99 | `linger.ms`,â€¯`batch.size` | â†‘â€¯for batching |
| Reduceâ€¯payload | `compression.type` | gzipâ€¯/â€¯zstd |
| Avoidâ€¯dupes | Idempotenceâ€¯/â€¯Txn | Enable |
| Fasterâ€¯reads | Cacheâ€¯hitâ€¯rate | â†‘ |

---

## ğŸ§  Checklist

- [ ] Identify **critical path**  
- [ ] Mark **sequential vs parallel**  
- [ ] Assign **numbers**  
- [ ] Compute **Î£â€¯orâ€¯max** vs SLA  
- [ ] Apply **Littleâ€™s Law**  
- [ ] Check **bandwidthâ€¯+â€¯payload**  
- [ ] Add **timeouts, retries, buffers**  
- [ ] Validate **P95/P99**

---

## ğŸ” Mermaid Oneâ€‘Pager

```mermaid
flowchart LR
  A[Define SLA] --> B[List Steps]
  B --> C{Sequential or Parallel?}
  C -- Seq --> D[Total = Sum]
  C -- Par --> E[Total = Max]
  D --> F[Compare to SLA]
  E --> F
  F --> G[Concurrency = RPS Ã— Latency]
  G --> H[Threads / Async Model]
  F --> I[Bandwidth = RPS Ã— Payload Ã— 8]
```

---

âœ… Paste this into any **systemâ€‘design or performance** repo to model latency, throughput, and scaling in seconds.

