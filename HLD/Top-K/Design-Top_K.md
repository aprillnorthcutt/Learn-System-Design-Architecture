# ğŸ§  Deep-Dive â€” Top-K Views Architecture (Kafka Â· Flink Â· Postgres Â· Redis)

> **Goal:** Find the most-viewed videos quickly (Top-K) with accurate counts, even when millions of people are watching at once.

----
 > **Func Req's:*** <br>
>  All-Time top-k videos (max 1000) <br>
> Tumbling 1 hour, 1 day, 1 month, all-time <br>

> **Non-Func Req's:** <br>
> 1s delay to writes <br>
> No approximations <br>
> Massive # of views (TBD) <br>
> Massive # of videos (TBD) <br>
> Results in 10-100ms <br>


----
> **Core Entities:** <br>
> Video <br>
> View <br>
> Time Window <br>

---

>
> **API Interface:** GET /views/top-k?window={WINDOW}&k={K} -> { videoId: string, views: number }[]
>
 ---

> **Solution:** We partition Kafka by videoId and key Flink by videoId, so each videoâ€™s stream is ordered and processed by a single parallel task.
> Flink aggregates by fixed windows and writes idempotent updates into Postgres. We index each window table on (window_start, views DESC) so Top-K is index-friendly. A small job precomputes Top-K lists into Redis; the API reads from cache for constant-time latency. If load rises, we add Kafka partitions, bump Flink parallelism, and shard Postgres by videoId, keeping the same key across the stack for simple, linearly scalable operations.â€
>
---


## ğŸ§© System Diagram
```mermaid
flowchart TD
  U["Clients (Web / Mobile / API)"] -->|"GET /top-k"| API["My API / Top-K Service (stateless)<br/>Serves precomputed results"]

  API -->|"Read"| REDIS["Redis Cache Cluster<br/>Precomputed Top-K lists, TTL ~30d"]
  API -->|"Fallback (cache miss)"| PGREAD["Postgres Read Replica"]

  EVT["ViewEvent Producers<br/>(Edge apps, web, mobile, CDN)"] --> KAFKA["Kafka Topic: view_events<br/>(partitioned by video_id)"]

  KAFKA --> FLINK["View Consumer (Flink)<br/>Aggregates + checkpoints every 5m"]
  FLINK -->|"Upsert counts"| PG["Views DB (Postgres)<br/>Aggregates: hour / day / month / all_time"]

  PG --> CRON["Top-K Cron / Operator<br/>Periodic Top-K SQL queries"]
  CRON -->|"SET Top-K keys"| REDIS

  API -. "rehydrate cache on miss" .-> PG
```

---

## 1ï¸âƒ£ **ViewEvent Producers â†’ Kafka**  
**Think of Kafka as a postal sorting center.**  
- Every time someone watches a video, a `ViewEvent` is created.  
- Instead of writing straight to the database (which would melt under millions of requests), each event is **sent to Kafka**, which queues and organizes them.  
- Kafka breaks these into **partitions** (like conveyor belts). Each belt only handles one subset of videos â€” maybe videos whose IDs end in certain numbers.  

ğŸ“¦ **Why this matters:**  
- If one video goes viral, only its partition gets busy â€” others stay smooth.  
- Kafka guarantees **ordering within each partition**, so event order stays consistent for that video.  
- Nothing is lost; Kafka stores data durably on disk until itâ€™s processed.

ğŸ§© **Beginner takeaway:**  
Kafka doesnâ€™t change your data â€” itâ€™s the â€œinboxâ€ that absorbs unpredictable spikes so nothing breaks.

---

## 2ï¸âƒ£ **Kafka â†’ Flink (View Consumer)**  
**Flink is your â€œreal-time accountant.â€**  
- It reads events from Kafka and counts them up in near-real-time.  
- But instead of counting forever, it groups data into **time windows** â€” for example: â€œall views between 9:00 and 10:00.â€  

ğŸ•’ **Windows**  
- **Tumbling windows:** fixed, non-overlapping blocks of time (like boxes on a timeline).  
- **Sliding windows:** overlapping windows (more detailed but more costly).  
Youâ€™re using tumbling windows â€” simple and scalable.  

ğŸš° **Watermarks & lateness:**  
- Watermarks tell Flink, â€œweâ€™ve probably received everything up to time T.â€  
- But if a view arrives late (say a mobile client was offline), Flink can still count it as long as itâ€™s within the **lateness buffer** (e.g., 60 seconds).  
- Once the lateness window closes, the count is final and Flink â€œsealsâ€ the window.

ğŸ§© **Beginner takeaway:**  
Flink constantly listens to Kafka, grouping views by video and time.  
It ensures no double counting and tolerates late events gracefully.

---

## 3ï¸âƒ£ **Flink â†’ Postgres (Views DB)**  
Once Flink finishes counting a window, it writes the totals to Postgres â€” your **source of truth.**

ğŸ§® **Example:**  
| window_start | video_id | views |
|---------------|-----------|-------|
| 2025-10-19T14:00 | vid_777 | 97,210 |

âš™ï¸ **The insert logic uses â€œupsertâ€:**  
```sql
INSERT INTO video_views_hour(window_start, video_id, views)
VALUES ($1,$2,$3)
ON CONFLICT (window_start, video_id)
DO UPDATE SET views = video_views_hour.views + EXCLUDED.views;
```
That means if the row already exists, it just increases the count â€” no duplicates.  
If Flink retries after a crash, no double counting happens.

ğŸ’¾ **Database structure:**  
- `video_views_hour` â€” rolling hourly stats.  
- `video_views_day` â€” daily aggregates.  
- `video_views_month` â€” monthly aggregates.  
- `video_views_all_time` â€” cumulative total.  

ğŸ§© **Beginner takeaway:**  
Postgres is where you store the **exact** results.  
Flink just streams data in; Postgres keeps it correct and durable.

---

## 4ï¸âƒ£ **Top-K Cron / Operator â†’ Redis**  
Now we want to make â€œTop 100 videos this hourâ€ queries fast.  
You **donâ€™t** want to run a `SELECT â€¦ ORDER BY â€¦ LIMIT 100` every time â€” itâ€™s slow and expensive.  

Thatâ€™s why we precompute.  
A small **cron job** (or a background process) runs every few minutes:
```sql
SELECT video_id, views
FROM video_views_hour
WHERE window_start = $1
ORDER BY views DESC
LIMIT 100;
```

It stores the result in **Redis** as a ready-to-serve JSON blob:  
```
topk:hour:2025-10-19T14:00 â†’ { items: [["v1",98123],["v7",97210], ...] }
topk:hour:current â†’ "2025-10-19T14:00"
```

ğŸ§  **Redis = memory cache**  
- Data is stored in RAM for ultra-fast reads.  
- You can set a **TTL** (time-to-live) like 30 days so old data expires.  
- Because Redis is so fast, it handles thousands of API requests per second.

ğŸ§© **Beginner takeaway:**  
The cron is a **pre-builder** â€” it prepares answers before users even ask.  
Redis is the **vending machine** â€” it delivers the answer instantly.

---

## 5ï¸âƒ£ **My API / Top-K Service â†’ Clients**  
When a user requests `/top-k?window=hour`, the flow is simple:
1. API asks Redis for the current window key.  
2. Fetches the list for that timestamp.  
3. Returns it directly.  

If Redis doesnâ€™t have it (rare):  
- API falls back to Postgres to compute it.  
- Then writes the result back into Redis for next time.  

ğŸš€ **Performance goals:**  
- Cache hits: < 10 ms latency.  
- Fallbacks: < 300 ms.  
- Hit rate: > 98 %.

ğŸ§© **Beginner takeaway:**  
Your API doesnâ€™t crunch numbers â€” it just looks up ready answers.

---

## 6ï¸âƒ£ **Why this pipeline works**
| Goal | How itâ€™s achieved |
|------|--------------------|
| Handle millions of events/sec | Kafka partitions + Flink parallelism |
| Accurate counts | Flinkâ€™s exactly-once + idempotent Postgres upserts |
| Low-latency reads | Precomputed results in Redis |
| Fault-tolerance | Flink checkpoints + Kafka replay |
| Simplicity | SQL source of truth (Postgres) |

---

## 7ï¸âƒ£ **Trade-offs**
| Component | Strength | Weakness |
|------------|-----------|-----------|
| Kafka | Durable buffer, scalable | Needs ops & monitoring |
| Flink | Real-time windowing, strong semantics | Stateful jobs are complex |
| Postgres | Easy queries, correctness | Sharding required at scale |
| Cron | Predictable, simple | Not real-time; slightly stale |
| Redis | Lightning fast | Uses memory; eventual freshness |

---

## ğŸ§© Analogy Recap

| System | Role | Analogy |
|---------|------|---------|
| Kafka | Inbox | Mail sorting center that queues all events |
| Flink | Aggregator | Accountant tallying counts per box of time |
| Postgres | Truth keeper | Ledger storing all verified totals |
| Cron | Scheduler | Assistant who prints out daily top lists |
| Redis | Cache | Front desk handing you the latest chart |
| API | Interface | Receptionist who fetches the report for you |

---

## ğŸ’¬ What to Say in an Interview

> â€œEach view event first lands in Kafka, so we can scale writes and replay safely.  
> Flink aggregates events in event-time tumbling windows and upserts counts into Postgres with idempotent writes.  
> A cron job periodically queries Top-K per window and caches those results in Redis.  
> Our API serves from Redis for constant-time reads, with fallback to Postgres if needed.  
> The design balances correctness and speed: event-time accuracy on write, sub-millisecond reads on serve.â€
